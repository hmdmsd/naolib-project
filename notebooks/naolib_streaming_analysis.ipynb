{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naolib Streaming Analysis\n",
    "\n",
    "This notebook performs streaming analysis on real-time Naolib transportation data. We'll implement two streaming analyses with time windows:\n",
    "\n",
    "1. Real-time average wait times per line over sliding windows\n",
    "2. Detection of unexpected delays and service disruptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.functions import col, explode, when, expr, to_timestamp, regexp_extract, window, current_timestamp\n",
    "from pyspark.sql.functions import avg, max, min, count, stddev, from_json, struct, lit\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType, BooleanType\n",
    "import json\n",
    "\n",
    "# Configure Spark\n",
    "conf = SparkConf() \\\n",
    "    .setAppName('NaolibStreamingAnalysis') \\\n",
    "    .setMaster('spark://spark:7077') \\\n",
    "    .set(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3\") \\\n",
    "    .set(\"spark.sql.shuffle.partitions\", \"10\")\n",
    "\n",
    "sc = SparkContext.getOrCreate(conf=conf)\n",
    "sql_context = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define the Schemas\n",
    "\n",
    "First, let's define schemas for the data we'll be reading from Kafka:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema for the ligne (line) nested structure\n",
    "ligne_schema = StructType([\n",
    "    StructField(\"numLigne\", StringType(), True),\n",
    "    StructField(\"typeLigne\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Schema for the arret (stop) nested structure\n",
    "arret_schema = StructType([\n",
    "    StructField(\"codeArret\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Schema for each arrival in the arrivals array\n",
    "arrival_schema = StructType([\n",
    "    StructField(\"sens\", StringType(), True),\n",
    "    StructField(\"terminus\", StringType(), True), \n",
    "    StructField(\"infotrafic\", BooleanType(), True),\n",
    "    StructField(\"temps\", StringType(), True),\n",
    "    StructField(\"dernierDepart\", StringType(), True),\n",
    "    StructField(\"tempsReel\", StringType(), True),\n",
    "    StructField(\"ligne\", ligne_schema, True),\n",
    "    StructField(\"arret\", arret_schema, True)\n",
    "])\n",
    "\n",
    "# Schema for the overall message\n",
    "message_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"stop_code\", StringType(), True),\n",
    "    StructField(\"stop_name\", StringType(), True),\n",
    "    StructField(\"arrivals\", ArrayType(arrival_schema), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Kafka Connection\n",
    "\n",
    "Now, let's set up the connection to Kafka to stream our real-time transportation data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kafka configuration\n",
    "kafka_bootstrap_servers = \"kafka:9092\"\n",
    "kafka_topic = \"naolib_realtime\"\n",
    "\n",
    "# Create a streaming DataFrame from Kafka\n",
    "df_kafka = sql_context.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Convert the binary value to string\n",
    "df_string = df_kafka.selectExpr(\"CAST(value AS STRING)\")\n",
    "\n",
    "# Parse the JSON string into structured data\n",
    "df_parsed = df_string \\\n",
    "    .select(from_json(col(\"value\"), message_schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "\n",
    "# Explode the arrivals array\n",
    "df_arrivals = df_parsed \\\n",
    "    .select(\n",
    "        to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\").alias(\"collection_timestamp\"),\n",
    "        col(\"stop_code\"),\n",
    "        col(\"stop_name\"),\n",
    "        explode(col(\"arrivals\")).alias(\"arrival\")\n",
    "    )\n",
    "\n",
    "# Extract fields from the arrival structure\n",
    "df_clean = df_arrivals \\\n",
    "    .select(\n",
    "        col(\"collection_timestamp\"),\n",
    "        col(\"stop_code\"),\n",
    "        col(\"stop_name\"),\n",
    "        col(\"arrival.sens\").alias(\"direction\"),\n",
    "        col(\"arrival.terminus\").alias(\"terminus\"),\n",
    "        col(\"arrival.temps\").alias(\"wait_time_text\"),\n",
    "        col(\"arrival.tempsReel\").alias(\"is_real_time\"),\n",
    "        col(\"arrival.ligne.numLigne\").alias(\"line_number\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"wait_time_minutes\",\n",
    "        when(col(\"wait_time_text\") == \"proche\", 0)\n",
    "        .when(col(\"wait_time_text\").rlike(\"^\\\\d+$\"), col(\"wait_time_text\").cast(\"int\"))\n",
    "        .when(col(\"wait_time_text\").rlike(\"^\\\\d+ min$\"), regexp_extract(col(\"wait_time_text\"), \"(\\\\d+)\", 1).cast(\"int\"))\n",
    "        .otherwise(None)\n",
    "    ) \\\n",
    "    .withColumn(\"processing_time\", current_timestamp())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Streaming Analysis 1: Real-time Average Wait Times\n",
    "\n",
    "Our first streaming analysis calculates real-time average wait times per line using sliding time windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out rows with null wait times\n",
    "df_valid_waits = df_clean.filter(col(\"wait_time_minutes\").isNotNull())\n",
    "\n",
    "# Add watermarking to handle late data\n",
    "df_watermarked = df_valid_waits.withWatermark(\"collection_timestamp\", \"10 minutes\")\n",
    "\n",
    "# Calculate average wait times over 10-minute windows, sliding every 2 minutes\n",
    "avg_wait_times = df_watermarked \\\n",
    "    .groupBy(\n",
    "        window(col(\"collection_timestamp\"), \"10 minutes\", \"2 minutes\"),\n",
    "        col(\"line_number\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        avg(\"wait_time_minutes\").alias(\"avg_wait_time\"),\n",
    "        count(\"*\").alias(\"observation_count\"),\n",
    "        stddev(\"wait_time_minutes\").alias(\"stddev_wait_time\")\n",
    "    ) \\\n",
    "    .filter(col(\"observation_count\") >= 3)  # Only include results with enough observations\n",
    "\n",
    "# Extract window start and end times for better readability\n",
    "avg_wait_times_readable = avg_wait_times \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"avg_wait_time\"),\n",
    "        col(\"observation_count\"),\n",
    "        col(\"stddev_wait_time\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"window_start\").desc(), col(\"avg_wait_time\").desc())\n",
    "\n",
    "# Output the results to the console\n",
    "query1 = avg_wait_times_readable \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 20) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Streaming Analysis 2: Real-time Delay Detection\n",
    "\n",
    "Our second streaming analysis detects unusual delays and potential service disruptions using time windows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, calculate historical average wait times per line (as baseline)\n",
    "# In a real system, this would come from a historical database\n",
    "# For simplicity, we'll define typical_wait_times at a fixed value for now\n",
    "typical_wait_time = 10  # minutes\n",
    "\n",
    "# Detect unusual delays (wait times significantly above normal)\n",
    "df_delay_detection = df_valid_waits \\\n",
    "    .withColumn(\n",
    "        \"is_delayed\",\n",
    "        col(\"wait_time_minutes\") > (typical_wait_time * 1.5)  # 50% above normal is considered unusual\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"delay_minutes\",\n",
    "        when(col(\"is_delayed\"), col(\"wait_time_minutes\") - typical_wait_time).otherwise(0)\n",
    "    ) \\\n",
    "    .withWatermark(\"collection_timestamp\", \"10 minutes\")\n",
    "\n",
    "# Aggregate delays by line and time window\n",
    "delay_alerts = df_delay_detection \\\n",
    "    .groupBy(\n",
    "        window(col(\"collection_timestamp\"), \"15 minutes\", \"5 minutes\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"stop_name\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        max(\"wait_time_minutes\").alias(\"max_wait_time\"),\n",
    "        avg(\"wait_time_minutes\").alias(\"avg_wait_time\"),\n",
    "        count(\"*\").alias(\"observation_count\"),\n",
    "        sum(when(col(\"is_delayed\"), 1).otherwise(0)).alias(\"delayed_count\"),\n",
    "        avg(\"delay_minutes\").alias(\"avg_delay_minutes\")\n",
    "    ) \\\n",
    "    .filter(col(\"delayed_count\") >= 2)  # At least 2 observations must show delays\n",
    "    \n",
    "# Calculate delay severity\n",
    "delay_alerts_with_severity = delay_alerts \\\n",
    "    .withColumn(\n",
    "        \"delay_severity\",\n",
    "        when(col(\"avg_delay_minutes\") > 20, \"SEVERE\")\n",
    "        .when(col(\"avg_delay_minutes\") > 10, \"MODERATE\")\n",
    "        .otherwise(\"MINOR\")\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"alert_message\",\n",
    "        expr(\"concat('DELAY ALERT: Line ', line_number, ' at ', stop_name, ' - ', delay_severity, ' delay of ', round(avg_delay_minutes, 1), ' minutes')\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"alert_window_start\"),\n",
    "        col(\"window.end\").alias(\"alert_window_end\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"stop_name\"),\n",
    "        col(\"avg_wait_time\"),\n",
    "        col(\"max_wait_time\"),\n",
    "        col(\"delayed_count\"),\n",
    "        col(\"observation_count\"),\n",
    "        col(\"avg_delay_minutes\"),\n",
    "        col(\"delay_severity\"),\n",
    "        col(\"alert_message\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"alert_window_start\").desc(), col(\"avg_delay_minutes\").desc())\n",
    "\n",
    "# Output the delay alerts to the console\n",
    "query2 = delay_alerts_with_severity \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Wait for Query Termination\n",
    "\n",
    "Let the streaming queries run until manually terminated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the first query\n",
    "try:\n",
    "    query1.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Query 1 terminated by user.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the second query\n",
    "try:\n",
    "    query2.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Query 2 terminated by user.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Stop the Queries (Run this when you want to stop the streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop the queries\n",
    "query1.stop()\n",
    "query2.stop()\n",
    "print(\"All streaming queries stopped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Service Disruption Pattern Detection (Bonus)\n",
    "\n",
    "Let's implement a more sophisticated streaming analysis to detect patterns that might indicate service disruptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a pattern detection model to identify potential service disruptions\n",
    "# We'll use consecutive abnormal wait times as indicators of service issues\n",
    "\n",
    "# Prepare the data with anomaly scores\n",
    "df_anomaly_detection = df_valid_waits \\\n",
    "    .withColumn(\n",
    "        \"wait_time_zscore\",  # Simplified z-score calculation with fixed standard deviation\n",
    "        (col(\"wait_time_minutes\") - typical_wait_time) / 5  # Using 5 min as a standard deviation approximation\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"is_abnormal\",\n",
    "        col(\"wait_time_zscore\") > 2  # More than 2 standard deviations is considered abnormal\n",
    "    ) \\\n",
    "    .withWatermark(\"collection_timestamp\", \"15 minutes\")\n",
    "\n",
    "# Group by line and 30-minute tumbling windows to detect disruption patterns\n",
    "disruption_detection = df_anomaly_detection \\\n",
    "    .groupBy(\n",
    "        window(col(\"collection_timestamp\"), \"30 minutes\"),\n",
    "        col(\"line_number\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total_observations\"),\n",
    "        sum(when(col(\"is_abnormal\"), 1).otherwise(0)).alias(\"abnormal_count\"),\n",
    "        avg(\"wait_time_minutes\").alias(\"avg_wait_time\"),\n",
    "        max(\"wait_time_minutes\").alias(\"max_wait_time\")\n",
    "    ) \\\n",
    "    .filter(col(\"total_observations\") >= 5)  # Need enough observations for reliable detection\n",
    "\n",
    "# Classify service status\n",
    "service_status = disruption_detection \\\n",
    "    .withColumn(\n",
    "        \"abnormal_percentage\",\n",
    "        (col(\"abnormal_count\") / col(\"total_observations\")) * 100\n",
    "    ) \\\n",
    "    .withColumn(\n",
    "        \"service_status\",\n",
    "        when(col(\"abnormal_percentage\") >= 70, \"MAJOR DISRUPTION\")\n",
    "        .when(col(\"abnormal_percentage\") >= 40, \"MINOR DISRUPTION\")\n",
    "        .when(col(\"abnormal_percentage\") >= 20, \"DEGRADED SERVICE\")\n",
    "        .otherwise(\"NORMAL SERVICE\")\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"line_number\"),\n",
    "        col(\"total_observations\"),\n",
    "        col(\"abnormal_count\"),\n",
    "        col(\"abnormal_percentage\"),\n",
    "        col(\"avg_wait_time\"),\n",
    "        col(\"max_wait_time\"),\n",
    "        col(\"service_status\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"window_start\").desc(), col(\"abnormal_percentage\").desc())\n",
    "\n",
    "# Output the service status to the console\n",
    "query3 = service_status \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .option(\"numRows\", 10) \\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wait for the third query\n",
    "try:\n",
    "    query3.awaitTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Query 3 terminated by user.\")\n",
    "    query3.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary of Streaming Analyses\n",
    "\n",
    "In this notebook, we've implemented three streaming analyses:\n",
    "\n",
    "1. **Real-time Average Wait Times**: Calculates and updates average wait times per line using 10-minute sliding windows with 2-minute slides. This provides a continuous view of the current state of the transportation system.\n",
    "\n",
    "2. **Real-time Delay Detection**: Identifies unusual delays at specific stops and lines using 15-minute windows sliding every 5 minutes. It applies severity classification (MINOR, MODERATE, SEVERE) based on the magnitude of delays.\n",
    "\n",
    "3. **Service Disruption Detection**: Uses a more sophisticated pattern recognition approach to detect potential service disruptions over 30-minute tumbling windows. It classifies the service status from NORMAL to MAJOR DISRUPTION based on the percentage of abnormal observations.\n",
    "\n",
    "These analyses provide valuable real-time insights into the Nantes public transportation system, which could be used by operators and passengers to make informed decisions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}